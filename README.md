# Multilingual-Hate-Speech-Classification
This project explores multilingual hate speech detection using transformer-based architectures — XLM-RoBERTa and mT5 — to classify text as hate or non-hate across multiple languages. It compares a discriminative classifier (XLM-RoBERTa) with a generative text-to-text model (mT5), offering both accuracy and interpretability.
